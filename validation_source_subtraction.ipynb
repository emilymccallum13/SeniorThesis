{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e66b088-a5ea-4e64-82b2-d0db9b78a3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "from scipy.ndimage import label, maximum_position, distance_transform_edt\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83117db0-3954-4a83-ae39-de9868d1b6cc",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564657e2-c296-405c-82df-b5825b947097",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to filter through potential sources and assess whether the candidate is a local maximum and has a sufficient radial profile\n",
    "\n",
    "def is_star_candidate(image, x, y, X_full, Y_full, box_size=7, inner_radius=3, outer_radius=7, min_contrast=2.0):\n",
    "\n",
    "    x_int = int(round(x))\n",
    "    y_int = int(round(y))\n",
    "    \n",
    "    half_box = box_size // 2\n",
    "    y_min = max(0, y_int - half_box)\n",
    "    y_max = min(image.shape[0], y_int + half_box + 1)\n",
    "    x_min = max(0, x_int - half_box)\n",
    "    x_max = min(image.shape[1], x_int + half_box + 1)\n",
    "    sub_box = image[y_min:y_max, x_min:x_max]\n",
    "    if np.nanmax(sub_box) != image[y_int, x_int]:\n",
    "        return False\n",
    "\n",
    "    local_y_min = max(0, y_int - outer_radius)\n",
    "    local_y_max = min(image.shape[0], y_int + outer_radius + 1)\n",
    "    local_x_min = max(0, x_int - outer_radius)\n",
    "    local_x_max = min(image.shape[1], x_int + outer_radius + 1)\n",
    "    Y_local = Y_full[local_y_min:local_y_max, local_x_min:local_x_max]\n",
    "    X_local = X_full[local_y_min:local_y_max, local_x_min:local_x_max]\n",
    "    \n",
    "    r = np.sqrt((X_local - x_int)**2 + (Y_local - y_int)**2)\n",
    "    annulus_mask = (r >= inner_radius) & (r < outer_radius)\n",
    "    if np.sum(annulus_mask) < 2:\n",
    "        return False\n",
    "    annulus_median = np.nanmedian(image[local_y_min:local_y_max, local_x_min:local_x_max][annulus_mask])\n",
    "    annulus_std = np.nanstd(image[local_y_min:local_y_max, local_x_min:local_x_max][annulus_mask])\n",
    "    \n",
    "    return image[y_int, x_int] > annulus_median + min_contrast * annulus_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560d4007-6037-41b2-b5a1-07fd2bdfce52",
   "metadata": {},
   "source": [
    "## Main Source Subtraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc869ad-f0f1-4d40-bfef-6c8784b1ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def psf_subtraction_cycle(detection_image, working_image, cycle_label, psf_data, output_dir,\n",
    "                          mosaic_header, permanent_mask, mask_distance, min_mask_sep=3,\n",
    "                          catalog_filename=None, threshold_sigma=3.0):\n",
    "\n",
    "    print(f\"\\n--- Starting Cycle {cycle_label} ---\")\n",
    "    \n",
    "    # Calculate Background Statistics\n",
    "    _, global_median, global_std = sigma_clipped_stats(detection_image, sigma=3.0)\n",
    "    threshold = global_median + threshold_sigma * global_std\n",
    "    print(f\"Cycle {cycle_label}: Global median = {global_median:.3f}, std = {global_std:.3f}, threshold = {threshold:.3f}\")\n",
    "    \n",
    "    # Identify Potential Sources and Positions\n",
    "    bmask = detection_image > threshold\n",
    "    labeled, num_regions = label(bmask)\n",
    "    print(f\"Cycle {cycle_label}: Number of connected regions above threshold: {num_regions}\")\n",
    "    \n",
    "    initial_positions = maximum_position(detection_image, labels=labeled,\n",
    "                                          index=np.arange(1, num_regions+1))\n",
    "\n",
    "    # Filter Potential Sources and Create Final Source List\n",
    "    source_list = []\n",
    "    Y_full, X_full = np.indices(detection_image.shape)\n",
    "    for pos in initial_positions:\n",
    "        y, x = pos\n",
    "        \n",
    "        if mask_distance[y, x] < min_mask_sep:\n",
    "            continue\n",
    "            \n",
    "        brightness = detection_image[y, x]\n",
    "        if np.isnan(brightness):\n",
    "            continue\n",
    "            \n",
    "        if is_star_candidate(detection_image, x, y, X_full, Y_full, box_size=1, inner_radius=1, outer_radius=2, min_contrast=0.5):\n",
    "            source_list.append((x, y, brightness))\n",
    "\n",
    "    # Sort Final List by Decreasing Brightness\n",
    "    source_list = sorted(source_list, key=lambda s: s[2], reverse=True)\n",
    "    \n",
    "    print(f\"Cycle {cycle_label}: {len(source_list)} candidate sources passed.\")\n",
    "\n",
    "    # Initialize List to Keep Track of Subtracted Sources\n",
    "    new_subtracted_sources = []\n",
    "    \n",
    "    # Define Magnitude Zero-Point\n",
    "    mag_zeropoint = 20.787  # Set by user based on mosaic in use\n",
    "\n",
    "    # Establish Header for Catalog - can be set by user\n",
    "    if catalog_filename is not None and not os.path.exists(catalog_filename):\n",
    "        with open(catalog_filename, \"w\") as f:\n",
    "            f.write(\"# Cycle  SourceIndex   X(pix)   Y(pix)   PeakDN   ScalingFactor   ABmag\\n\")\n",
    "            f.write(\"# -----------------------------------------------------------------------\\n\")\n",
    "    \n",
    "    # Iterate Over Each Source\n",
    "    for i, (x, y, bright) in enumerate(source_list, start=1):\n",
    "        \n",
    "        psf_shape = psf_data.shape\n",
    "        half_psf_x = psf_shape[1] // 2\n",
    "        half_psf_y = psf_shape[0] // 2\n",
    "        x_int, y_int = int(round(x)), int(round(y))\n",
    "        xmin = max(0, x_int - half_psf_x)\n",
    "        xmax = min(working_image.shape[1], x_int + half_psf_x)\n",
    "        ymin = max(0, y_int - half_psf_y)\n",
    "        ymax = min(working_image.shape[0], y_int + half_psf_y)\n",
    "        \n",
    "        # Extract Region Corresponding to PSF Model\n",
    "        region = working_image[ymin:ymax, xmin:xmax]\n",
    "        psf_resized = psf_data[:region.shape[0], :region.shape[1]]\n",
    "        \n",
    "        # Calculate PSF Scaling Factor\n",
    "        s = np.nansum(region * psf_resized) / np.nansum(psf_resized**2)\n",
    "        \n",
    "        # Compute AB Magnitude \n",
    "        if bright > 0:\n",
    "            peak_mag = -2.5 * np.log10(s) + mag_zeropoint\n",
    "        else:\n",
    "            # Dummy value to save if brightness is not a positive value\n",
    "            peak_mag = 99.999\n",
    "        \n",
    "        # Save Statistics to Catalog\n",
    "        if catalog_filename is not None:\n",
    "            with open(catalog_filename, \"a\") as f:\n",
    "                f.write(f\"{cycle_label:<5d} {i:<12d} {x:9.1f} {y:8.1f} {bright:9.2f} {s:14.3f} {peak_mag:9.3f}\\n\")\n",
    "        \n",
    "        # PSF Subtraction\n",
    "        working_image[ymin:ymax, xmin:xmax] -= s * psf_resized\n",
    "\n",
    "        # Save Subtracted Source to List\n",
    "        new_subtracted_sources.append((x, y))\n",
    "    \n",
    "    # Save Updated Residual Image After Each Iteration\n",
    "    out_file = os.path.join(output_dir, f'final_psf_subtracted_cycle{cycle_label}.fits')\n",
    "    final_display = np.nan_to_num(working_image, nan=0)\n",
    "    fits.PrimaryHDU(final_display, header=mosaic_header).writeto(out_file, overwrite=True)\n",
    "    print(f\"Cycle {cycle_label}: Saved PSF-subtracted image as {out_file}\")\n",
    "    \n",
    "    # Write Total Number of Sources Subtracted in this Cycle to Catalog\n",
    "    if catalog_filename is not None:\n",
    "        with open(catalog_filename, \"a\") as f:\n",
    "            f.write(f\"# Cycle {cycle_label}: {len(new_subtracted_sources)} sources subtracted\\n\")\n",
    "    \n",
    "    return working_image, new_subtracted_sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0149c5b-7be5-44f6-b4d7-86298f9ca390",
   "metadata": {},
   "source": [
    "## Validation Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07fd348-3582-4c75-b20b-10213fb22a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to Insert Artificial Sources into Mosaic for Testing\n",
    "\n",
    "def inject_fake_sources(image, psf_data, n_sources=250, scaling_factors=[5, 10, 15, 20]):\n",
    "    np.random.seed(None)\n",
    "    injected_image = image.copy()\n",
    "    injected_catalog = []\n",
    "    psf_shape = psf_data.shape\n",
    "    half_psf_x = psf_shape[1] // 2\n",
    "    half_psf_y = psf_shape[0] // 2\n",
    "    ny, nx = image.shape\n",
    "    x_min, x_max = half_psf_x, nx - half_psf_x - 1\n",
    "    y_min, y_max = half_psf_y, ny - half_psf_y - 1\n",
    "    \n",
    "    for scale_factor in scaling_factors:\n",
    "        for i in range(n_sources):\n",
    "            x = np.random.randint(x_min, x_max)\n",
    "            y = np.random.randint(y_min, y_max)\n",
    "            \n",
    "            x_start = x - half_psf_x\n",
    "            x_end = x_start + psf_shape[1]\n",
    "            y_start = y - half_psf_y\n",
    "            y_end = y_start + psf_shape[0]\n",
    "            injected_image[y_start:y_end, x_start:x_end] += scale_factor * psf_data\n",
    "            \n",
    "            injected_catalog.append((x, y, scale_factor))\n",
    "    return injected_image, injected_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a42ab56-0699-4b5b-844e-2a5c899dc9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to Identify Matched and Unmatched Artificial Sources \n",
    "\n",
    "def match_catalogs(injected_catalog, detected_catalog, max_sep=5.0):\n",
    "    matches = []\n",
    "    unmatched_injected = []\n",
    "    for inj in injected_catalog:\n",
    "        inj_x, inj_y, inj_scale = inj\n",
    "        best_dist = np.inf\n",
    "        best_det = None\n",
    "        for det in detected_catalog:\n",
    "            det_x, det_y, det_val = det\n",
    "            dist = np.sqrt((inj_x - det_x)**2 + (inj_y - det_y)**2)\n",
    "            if dist < best_dist:\n",
    "                best_dist = dist\n",
    "                best_det = det\n",
    "        if best_dist <= max_sep:\n",
    "            matches.append((inj, best_det, best_dist))\n",
    "        else:\n",
    "            unmatched_injected.append(inj)\n",
    "    return matches, unmatched_injected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110104fe-f75c-47ee-be32-0cc2cd8d8057",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to Measure and Plot Completeness, and Save Plot to Output Directory\n",
    "\n",
    "def plot_completeness(matches, injected_catalog, brightness_bins, mosaic_type='4.5', output_dir=None):\n",
    "\n",
    "    # mosaic type between channels; can be reset by user\n",
    "    if mosaic_type == '3.6':\n",
    "        zeropoint = 20.787\n",
    "    elif mosaic_type == '4.5':\n",
    "        zeropoint = 20.798\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mosaic type. Choose '3.6' or '4.5'.\")\n",
    "\n",
    "    # scaling factors\n",
    "    inj_scales = np.array([src[2] for src in injected_catalog])\n",
    "    det_scales = np.array([det[2] for (_, det, _) in matches]) \n",
    "\n",
    "    counts_injected, bins = np.histogram(inj_scales, bins=brightness_bins)\n",
    "    counts_matched, _ = np.histogram(det_scales, bins=brightness_bins)\n",
    "    completeness = counts_matched / np.maximum(counts_injected, 1)\n",
    "\n",
    "    bin_centers = 0.5 * (bins[1:] + bins[:-1])\n",
    "    mag_bin_centers = -2.5 * np.log10(bin_centers) + zeropoint\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(mag_bin_centers, completeness, 'o-', label='Completeness')\n",
    "    plt.xlabel('Injected Magnitude')\n",
    "    plt.ylabel('Completeness')\n",
    "    plt.title('Completeness vs. Injected Magnitude')\n",
    "    plt.ylim(0,1.1)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.gca().invert_xaxis()\n",
    "    if output_dir is not None:\n",
    "        plt.savefig(os.path.join(output_dir, \"completeness_vs_magnitude.png\"), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed69f21e-2a71-4410-ab9e-78b2605f9ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to Measure and Plot Photometric Error, and Save Plot to Output Directory\n",
    "\n",
    "def plot_photometric_error(matches, mosaic_type='4.5', output_dir=None):\n",
    "\n",
    "    # mosaic type between channels; can be reset by user\n",
    "    if mosaic_type == '3.6':\n",
    "        zeropoint = 20.787\n",
    "    elif mosaic_type == '4.5':\n",
    "        zeropoint = 20.798\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mosaic type. Choose '3.6' or '4.5'.\")\n",
    "\n",
    "    # scaling factors\n",
    "    inj_scales = np.array([inj[2] for (inj, det, sep) in matches])\n",
    "    fit_scales = np.array([det[2] for (inj, det, sep) in matches])\n",
    "\n",
    "    true_m = -2.5 * np.log10(inj_scales) + zeropoint\n",
    "    calc_m = -2.5 * np.log10(fit_scales) + zeropoint\n",
    "    errors = (true_m - calc_m) / true_m\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(true_m, errors, 'o', label='Fractional Error')\n",
    "    plt.xlabel('Magnitude')\n",
    "    plt.ylabel('Photometric Error')\n",
    "    plt.title('Photometric Accuracy vs. Test Source Magnitude')\n",
    "    plt.axhline(0, color='gray', linestyle='--')\n",
    "    plt.grid(True)\n",
    "    if output_dir is not None:\n",
    "        plt.savefig(os.path.join(output_dir, \"photometric_accuracy_vs_magnitude.png\"), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a81614-b848-46af-9fec-8cd18cfc2964",
   "metadata": {},
   "source": [
    "## Main Iterative Workflow and Validation Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6764e9e2-830b-4eba-bd16-8ad51416bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to be Set by User\n",
    "mosaic_file = 'mosaic_file'\n",
    "psf_file = 'psf_file'\n",
    "output_dir = 'output_directory'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Establish Catalog\n",
    "catalog_filename = os.path.join(output_dir, \"source_catalog.tex\")\n",
    "\n",
    "# Load Data\n",
    "with fits.open(mosaic_file) as hdu:\n",
    "    data = np.copy(hdu[0].data)\n",
    "    mosaic_header = hdu[0].header\n",
    "\n",
    "with fits.open(psf_file) as hdu_psf:\n",
    "    psf_data = np.copy(hdu_psf[0].data)\n",
    "\n",
    "# Print Mosaic Dimensions\n",
    "ny, nx = data.shape\n",
    "print(f\"Mosaic dimensions: {nx} x {ny} pixels\")\n",
    "\n",
    "# Define Submosaic Cutout to Validate On\n",
    "cut_size = 4000\n",
    "if cut_size > nx or cut_size > ny:\n",
    "    raise ValueError(\"cut_size is larger than the mosaic dimensions.\")\n",
    "center_x = nx // 2\n",
    "center_y = ny // 2\n",
    "half_cut = cut_size // 2\n",
    "sub_mosaic = data[center_y-half_cut:center_y+half_cut, center_x-half_cut:center_x+half_cut]\n",
    "print(f\"Sub-mosaic dimensions: {sub_mosaic.shape}\")\n",
    "\n",
    "# Save Submosaic to Output Directory\n",
    "sub_mosaic_file = os.path.join(output_dir, 'sub_mosaic.fits')\n",
    "fits.PrimaryHDU(sub_mosaic, header=mosaic_header).writeto(sub_mosaic_file, overwrite=True)\n",
    "print(f\"Saved sub_mosaic as {sub_mosaic_file}\")\n",
    "\n",
    "### Routine to Mask Out Saturated Pixels\n",
    "\n",
    "saturation_threshold = 60.0              # saturation threshold; to be adjusted by user\n",
    "small_initial_mask_radius = 160          # small mask radius; to be adjusted by user  \n",
    "large_initial_mask_radius = 220          # large mask radius; to be adjusted by user \n",
    "\n",
    "# Define Global Background\n",
    "global_bg = np.nanmedian(sub_mosaic)\n",
    "\n",
    "# Identify Saturated Regions\n",
    "saturated_pixels = sub_mosaic > saturation_threshold\n",
    "labeled_saturated, num_sat = label(saturated_pixels)\n",
    "print(f\"Number of saturated regions detected: {num_sat}\")\n",
    "\n",
    "# Iterate Through Saturated Regions and Collect Statistics\n",
    "region_info = []\n",
    "for region_label in range(1, num_sat+1):\n",
    "    y_indices, x_indices = np.where(labeled_saturated == region_label)\n",
    "    if len(y_indices) == 0:\n",
    "        continue\n",
    "    region_values = sub_mosaic[y_indices, x_indices]\n",
    "    if np.nanmax(region_values) < min_flux_threshold:\n",
    "        continue\n",
    "    peak_index = np.argmax(region_values)\n",
    "    y_peak, x_peak = y_indices[peak_index], x_indices[peak_index]\n",
    "    distances_region = np.sqrt((x_indices - x_peak)**2 + (y_indices - y_peak)**2)\n",
    "    effective_radius = np.max(distances_region)\n",
    "    x_min_box, x_max_box = np.min(x_indices), np.max(x_indices)\n",
    "    y_min_box, y_max_box = np.min(y_indices), np.max(y_indices)\n",
    "    width = x_max_box - x_min_box + 1\n",
    "    height = y_max_box - y_min_box + 1\n",
    "    aspect_ratio = width / height if height != 0 else 1.0\n",
    "    peak_flux = np.nanmax(region_values)\n",
    "    region_info.append((region_label, y_peak, x_peak, effective_radius, aspect_ratio, peak_flux))\n",
    "\n",
    "# Calculate Average Effective Radius of Saturated Regions\n",
    "if region_info:\n",
    "    avg_effective_radius = np.mean([info[3] for info in region_info])\n",
    "else:\n",
    "    avg_effective_radius = 0\n",
    "print(f\"Average effective radius = {avg_effective_radius:.1f} pixels\")\n",
    "\n",
    "# Set Mask Size for Each Region Based on Effective Radius\n",
    "for (region_label, y_peak, x_peak, effective_radius, aspect_ratio, peak_flux) in region_info:\n",
    "    if effective_radius <= avg_effective_radius * 1.5:\n",
    "        r_final = small_initial_mask_radius\n",
    "    else:\n",
    "        r_final = large_initial_mask_radius\n",
    "    Y, X = np.indices(sub_mosaic.shape)\n",
    "    distance_arr = np.sqrt((X - x_peak)**2 + (Y - y_peak)**2)\n",
    "    final_mask = distance_arr <= r_final\n",
    "    sub_mosaic[final_mask] = np.nan\n",
    "\n",
    "permanent_mask = np.isnan(sub_mosaic)\n",
    "\n",
    "# Sets Mask Separation Distance - Only Sources At Least the 'min_mask_sep' Distance from Masked Regions Will be Considered\n",
    "mask_distance = distance_transform_edt(~permanent_mask)\n",
    "min_mask_sep = 2\n",
    "\n",
    "# Make Copies of Sub-Mosaic and Masked Image; Save Masked Image to Output Directory\n",
    "original_sub_mosaic = sub_mosaic.copy()\n",
    "masked_image = original_sub_mosaic.copy()\n",
    "masked_image[permanent_mask] = 0\n",
    "masked_image_file = os.path.join(output_dir, \"masked_image.fits\")\n",
    "fits.PrimaryHDU(masked_image, header=mosaic_header).writeto(masked_image_file, overwrite=True)\n",
    "print(f\"Masked image saved as {masked_image_file}\")\n",
    "\n",
    "\n",
    "### Add in Artificial Sources \n",
    "\n",
    "scaling_factors = [2, 6, 12, 24]  # scaling factors to be set by user\n",
    "\n",
    "# Number of sources to be added (n_sources) to be set by user\n",
    "test_image, injected_catalog = inject_fake_sources(sub_mosaic, psf_data, n_sources=300, scaling_factors=scaling_factors)\n",
    "print(f\"Injected {len(injected_catalog)} fake sources for validation.\")\n",
    "\n",
    "# Save Test Image to Output Directory\n",
    "test_image_file = os.path.join(output_dir, \"test_image_with_injected_sources.fits\")\n",
    "fits.PrimaryHDU(test_image, header=mosaic_header).writeto(test_image_file, overwrite=True)\n",
    "print(f\"Test image with fake sources saved as {test_image_file}\")\n",
    "\n",
    "# Get Number of Artificial Sources that Fall in Masked Regions\n",
    "masked_counts = {val: 0 for val in scaling_factors}\n",
    "nonmasked_counts = {val: 0 for val in scaling_factors}\n",
    "psf_shape = psf_data.shape\n",
    "half_psf_x = psf_shape[1] // 2\n",
    "half_psf_y = psf_shape[0] // 2\n",
    "\n",
    "for (x, y, scale_factor) in injected_catalog:\n",
    "    x_start = x - half_psf_x\n",
    "    x_end = x_start + psf_shape[1]\n",
    "    y_start = y - half_psf_y\n",
    "    y_end = y_start + psf_shape[0]\n",
    "    if np.all(permanent_mask[y_start:y_end, x_start:x_end]):\n",
    "        masked_counts[scale_factor] += 1\n",
    "    else:\n",
    "        nonmasked_counts[scale_factor] += 1\n",
    "\n",
    "print(\"Artificial Sources Masked Counts:\")\n",
    "for val in scaling_factors:\n",
    "    print(f\"Scale {val}: Masked: {masked_counts[val]}, Non-masked: {nonmasked_counts[val]}\")\n",
    "\n",
    "with open(catalog_filename, \"a\") as f:\n",
    "    f.write(\"%% Artificial Sources Masked Counts:\\n\")\n",
    "    for val in scaling_factors:\n",
    "        f.write(f\"Scaling {val}: Masked: {masked_counts[val]}, Non-masked: {nonmasked_counts[val]} \\\\\\\\\\n\")\n",
    "\n",
    "# Display Image with Marked Artificial Sources Overlaid\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(test_image, origin='lower', cmap='gray')\n",
    "color_map = {2: 'blue', 6: 'green', 12: 'orange', 24: 'red'}  # should match scaling_factors\n",
    "\n",
    "for val in sorted(color_map.keys()):\n",
    "    xs = [x for (x, y, s) in injected_catalog if s == val]\n",
    "    ys = [y for (x, y, s) in injected_catalog if s == val]\n",
    "    plt.plot(xs, ys, marker='x', linestyle='None', color=color_map[val],\n",
    "             markersize=10, markeredgewidth=2, label=f'Scale {val}')\n",
    "plt.title(\"Test Image with Injected Fake Sources Marked\")\n",
    "plt.colorbar()\n",
    "plt.legend()\n",
    "marked_file = os.path.join(output_dir, \"test_image_with_injected_sources_marked.png\")\n",
    "plt.savefig(marked_file, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save Catalog of Artificial Sources to CSV\n",
    "catalog_file = os.path.join(output_dir, \"injected_sources_catalog.csv\")\n",
    "with open(catalog_file, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"X\", \"Y\", \"ScalingFactor\"])\n",
    "    for x, y, scale_factor in injected_catalog:\n",
    "        writer.writerow([x, y, scale_factor])\n",
    "print(\"Injected sources catalog saved as\", catalog_file)\n",
    "\n",
    "# Define Bins for the Magnitudes\n",
    "brightness_bins = np.array([1, 4, 10, 16, 26])\n",
    "brightness_keys = [2, 6, 12, 24]\n",
    "completeness_by_bin = {k: [] for k in brightness_keys}\n",
    "normalized_completeness_by_bin = {k: [] for k in brightness_keys}\n",
    "photometric_error_by_bin = {k: [] for k in brightness_keys}\n",
    "detection_counts = []\n",
    "\n",
    "### Iterative Cycle Set Up\n",
    "\n",
    "ncycles = 10   # number of cycles: set by user (should be higher to ensure proper validation)\n",
    "\n",
    "cumulative_working = test_image.copy()\n",
    "cumulative_exclusion_mask = np.zeros_like(test_image, dtype=bool)\n",
    "all_new_sources = []\n",
    "\n",
    "zeropoint = 20.798  # zero point value to be set by user\n",
    "\n",
    "# Iterate Through All Cycles\n",
    "for cycle in range(1, ncycles+1):\n",
    "    print(f\"\\n========== Starting Iterative Cycle {cycle} ==========\")\n",
    "    detection_image = cumulative_working.copy()\n",
    "    detection_image[cumulative_exclusion_mask] = np.nan\n",
    "    \n",
    "    if cycle == 1:\n",
    "        tsig = 5.0      # threshold above background; set by user\n",
    "    else:\n",
    "        tsig = 3.0      # optional; lower threshold after first iteration\n",
    "\n",
    "    # PSF Subtraction Function\n",
    "    cumulative_working, new_sources = psf_subtraction_cycle(\n",
    "        detection_image, cumulative_working, cycle, psf_data, output_dir,\n",
    "        mosaic_header, permanent_mask, mask_distance, min_mask_sep,\n",
    "        catalog_filename, threshold_sigma=tsig)\n",
    "    all_new_sources.extend(new_sources)\n",
    "    \n",
    "    # Update Exclusion Mask; Prevents Re-Subtraction\n",
    "    box_half = 1 # set size for each source\n",
    "    for (x, y, s) in new_sources:\n",
    "        x_center = int(round(x))\n",
    "        y_center = int(round(y))\n",
    "        x_min_box = max(0, x_center - box_half)\n",
    "        x_max_box = min(test_image.shape[1], x_center + box_half + 1)\n",
    "        y_min_box = max(0, y_center - box_half)\n",
    "        y_max_box = min(test_image.shape[0], y_center + box_half + 1)\n",
    "        box = cumulative_working[y_min_box:y_max_box, x_min_box:x_max_box]\n",
    "        if np.all(np.isnan(box)):\n",
    "            continue\n",
    "        flat = box.flatten()\n",
    "        valid = np.where(~np.isnan(flat))[0]\n",
    "        if len(valid) == 0:\n",
    "            continue\n",
    "        sorted_idx = valid[np.argsort(flat[valid])[::-1]]\n",
    "        for idx in sorted_idx[:3]:\n",
    "            row = y_min_box + idx // (x_max_box - x_min_box)\n",
    "            col = x_min_box + idx % (x_max_box - x_min_box)\n",
    "            cumulative_exclusion_mask[row, col] = True\n",
    "\n",
    "    detected_catalog_cycle = all_new_sources.copy()\n",
    "    detection_counts.append(len(detected_catalog_cycle))\n",
    "    \n",
    "    # Match Detected Sources with Artificial Sources to Determine Which Artificial Sources Were Identified\n",
    "    matches_cycle, unmatched_cycle = match_catalogs(injected_catalog, detected_catalog_cycle, max_sep=5.0)\n",
    "    purity_cycle = len(matches_cycle) / len(detected_catalog_cycle) if len(detected_catalog_cycle) > 0 else 0\n",
    "    print(f\"Cycle {cycle} Metrics:\")\n",
    "    print(f\"  Cumulative fitted sources: {len(detected_catalog_cycle)}\")\n",
    "    print(f\"  Matched sources: {len(matches_cycle)}\")\n",
    "    print(f\"  Purity: {purity_cycle:.2f}\")\n",
    "    \n",
    "    # Calculate Completeness per Magnitude Bin\n",
    "    for i, (low, high) in enumerate(zip(brightness_bins[:-1], brightness_bins[1:])):\n",
    "        key = brightness_keys[i]\n",
    "        inj_in_bin = [src for src in injected_catalog if low <= src[2] < high]\n",
    "        n_injected = len(inj_in_bin)\n",
    "        matched_in_bin = [m for m in matches_cycle if low <= m[0][2] < high]\n",
    "        n_matched = len(matched_in_bin)\n",
    "        comp = n_matched / n_injected if n_injected > 0 else np.nan\n",
    "        completeness_by_bin[key].append(comp)\n",
    "        \n",
    "        # Normalized Completeness - Normalizes over all Non-Masked Artificial Sources (Since Those Over Masks Cannot be Detected)\n",
    "        norm_comp = n_matched / nonmasked_counts[key] if nonmasked_counts[key] > 0 else np.nan\n",
    "        normalized_completeness_by_bin[key].append(norm_comp)\n",
    "    \n",
    "        # Calculate Photometric Error per Magnitude Bin\n",
    "        errors = []\n",
    "        for match in matched_in_bin:\n",
    "            inj_scale = match[0][2]\n",
    "            fit_scale = match[1][2]\n",
    "            # Magnitude from scaling_factor\n",
    "            true_m = -2.5 * np.log10(inj_scale) + zeropoint\n",
    "            calc_m = -2.5 * np.log10(fit_scale) + zeropoint\n",
    "            error = (true_m - calc_m) / true_m\n",
    "            errors.append(error)\n",
    "        avg_error = np.mean(errors) if errors else np.nan\n",
    "        photometric_error_by_bin[key].append(avg_error)\n",
    "\n",
    "    # Save Statistics\n",
    "    metrics_file = os.path.join(output_dir, f\"metrics_cycle{cycle}.txt\")\n",
    "    with open(metrics_file, \"w\") as mf:\n",
    "        mf.write(f\"Cycle {cycle} Metrics:\\n\")\n",
    "        mf.write(f\"  Cumulative fitted sources: {len(detected_catalog_cycle)}\\n\")\n",
    "        mf.write(f\"  Matched sources: {len(matches_cycle)}\\n\")\n",
    "    print(f\"Cycle {cycle} metrics saved as {metrics_file}\")\n",
    "\n",
    "# Plot Cumulative Detection Counts and Save Plot\n",
    "cycles = np.arange(1, ncycles+1)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(cycles, detection_counts, 'o-', label='Cumulative Fitted Sources per Iteration')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.ylabel('Number of Fitted Sources')\n",
    "plt.title('Cumulative Detection Count per Iteration')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "cumulative_detection_file = os.path.join(output_dir, \"cumulative_detection_count.png\")\n",
    "plt.savefig(cumulative_detection_file, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Cumulative detection count plot saved as\", cumulative_detection_file)\n",
    "\n",
    "# Plot Completeness vs. Iteration Per Bin and Save Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "for key in brightness_keys:\n",
    "    mag_value = -2.5 * np.log10(key) + zeropoint\n",
    "    plt.plot(cycles, completeness_by_bin[key], marker='o', label=f'Mag {mag_value:.2f}')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.ylabel('Cumulative Completeness')\n",
    "plt.title('Cumulative Completeness vs. Iteration (Scaling Factor Bins)')\n",
    "plt.ylim(0,1.1)\n",
    "plt.legend(title='Magnitude')\n",
    "plt.grid(True)\n",
    "comp_cycle_file = os.path.join(output_dir, \"completeness_vs_cycle_by_flux.png\")\n",
    "plt.savefig(comp_cycle_file, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Per-bin completeness plot saved as\", comp_cycle_file)\n",
    "\n",
    "# Plot Normalized Completeness vs. Iteration and Save Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "for key in brightness_keys:\n",
    "    mag_value = -2.5 * np.log10(key) + zeropoint\n",
    "    plt.plot(cycles, normalized_completeness_by_bin[key], marker='o', label=f'Mag {mag_value:.2f}')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.ylabel('Normalized Completeness')\n",
    "plt.title('Normalized Completeness vs. Iteration for Each Magnitude Bin')\n",
    "plt.legend(title='Magnitude')\n",
    "plt.grid(True)\n",
    "norm_comp_cycle_file = os.path.join(output_dir, \"normalized_completeness_vs_cycle.png\")\n",
    "plt.savefig(norm_comp_cycle_file, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Normalized completeness plot saved as\", norm_comp_cycle_file)\n",
    "\n",
    "# Plot Photometric Error vs. Iteration and Save Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "for key in brightness_keys:\n",
    "    mag_value = -2.5 * np.log10(key) + zeropoint\n",
    "    plt.plot(cycles, photometric_error_by_bin[key], marker='o', label=f'Mag {mag_value:.2f}')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.ylabel('Average Fractional Photometric Error')\n",
    "plt.title('Average Photometric Error vs. Iteration per Magnitude Bin')\n",
    "plt.legend(title='Magnitude')\n",
    "plt.grid(True)\n",
    "phot_cycle_file = os.path.join(output_dir, \"photometric_error_vs_cycle_by_flux.png\")\n",
    "plt.savefig(phot_cycle_file, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Per-bin photometric error plot saved as\", phot_cycle_file)\n",
    "\n",
    "# Print Final Matched Statistics\n",
    "print(f\"Overall: Total cumulative fitted sources: {len(all_new_sources)}\")\n",
    "matches_overall, unmatched_overall = match_catalogs(injected_catalog, all_new_sources, max_sep=5.0)\n",
    "print(f\"Number of artificial sources: {len(injected_catalog)}\")\n",
    "print(f\"Number of matched sources: {len(matches_overall)}\")\n",
    "print(f\"Number of missed artificial sources: {len(unmatched_overall)}\")\n",
    "\n",
    "print(\"Final counts per magnitude:\")\n",
    "for val in brightness_keys:\n",
    "    total_injected = sum(1 for src in injected_catalog if src[2] == val)\n",
    "    total_matched = sum(1 for (inj, det, sep) in matches_overall if inj[2] == val)\n",
    "    total_unmatched = total_injected - total_matched\n",
    "    print(f\"Scale {val}: Artificial: {total_injected}, Matched: {total_matched}, Unmatched: {total_unmatched}\")\n",
    "\n",
    "plot_completeness(matches_overall, injected_catalog, brightness_bins=brightness_bins, mosaic_type='4.5', output_dir=output_dir)\n",
    "plot_photometric_error(matches_overall, mosaic_type='4.5', output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0993a1-96d6-451d-b227-4da71bec06ee",
   "metadata": {},
   "source": [
    "The above Source Subtraction and Source Counts Validation Code was developed by Emily McCallum as part of her Applied Mathematics Senior Thesis at Harvard College. Latest update: 27 Mar 2025"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
